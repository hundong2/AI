사용하고 계신 시스템 사양을 기준으로 추론 모델을 추천해 드리겠습니다.

---

### 시스템 사양 분석

제공해주신 `nvidia-smi` 및 `free -h` 출력 결과는 다음과 같습니다.

* **GPU**: NVIDIA GeForce RTX 3060 Ti
    * **총 VRAM**: 8192MiB (약 **8GB**)
    * **사용 가능 VRAM**: 약 6.1GB (8192MiB - 2066MiB)
* **CPU**: 정보 없음.
* **RAM**:
    * **총 RAM**: 15GiB (약 **15GB**)
    * **사용 가능 RAM**: 약 12GiB

종합적으로, GPU의 8GB VRAM이 모델 추론 성능에 가장 큰 제약 조건이 됩니다. 모델을 GPU에 올리려면 VRAM이 충분해야 하며, 일반적인 모델은 파라미터 크기 외에 1~2GB의 오버헤드가 필요합니다.

---

### 1. 적절한 추론 모델 크기 산정

대규모 언어 모델(LLM)의 크기는 **파라미터 수**와 **양자화(quantization)** 수준에 따라 결정됩니다.

* **파라미터 수 (B)**: 모델의 크기를 나타냅니다. `7B`는 70억 개의 파라미터를 의미합니다.
* **양자화**: 모델의 정밀도를 낮춰 메모리 사용량을 줄이는 기술입니다. `Q4`는 4비트로 양자화, `Q8`은 8비트로 양자화했다는 의미입니다. 양자화 레벨이 낮을수록(e.g., Q4 < Q8) 모델 크기는 작아지지만, 성능 저하가 있을 수 있습니다.

#### 산정 근거

대부분의 모델은 **`파라미터 크기(B) * 양자화 바이트`**로 대략적인 VRAM 용량을 계산할 수 있습니다. 예를 들어, 7B 모델을 Q4 양자화하면 약 4GB의 메모리를 사용합니다.

* 7B (4비트) 모델: `7B * 0.5 바이트/비트 = 3.5 GB`
* 7B (8비트) 모델: `7B * 1 바이트/비트 = 7 GB`

현재 시스템의 **사용 가능 VRAM은 약 6GB**이므로, **7B 파라미터**를 가진 모델 중 **Q8 이하**의 양자화 버전을 사용하는 것이 가장 적합합니다. 8B 모델도 낮은 양자화(`Q4` 또는 `Q5`) 버전을 사용하면 충분히 올릴 수 있습니다.

---

### 2. 추천 모델 리스트

아래는 현재 시스템에 적합한 추론 모델, 임베딩 모델, 그리고 VLM(Vision-Language Model) 모델 추천입니다.

#### 1) LLM (언어 모델)

* **Llama 3 8B**: Meta에서 출시한 최신 모델로, 8B 파라미터는 7B 모델보다 약간 크지만, 성능이 매우 뛰어납니다. `Q4_K_M` (약 5GB) 또는 `Q5_K_M` (약 5.5GB) 버전을 사용하면 GPU에서 원활하게 추론할 수 있습니다.
* **Mistral 7B Instruct**: 7.3B 파라미터로, 매우 빠른 속도와 뛰어난 성능을 자랑합니다. `Q8_0` (약 7.5GB) 버전까지 GPU에 올릴 수 있어, 최고의 성능을 경험할 수 있습니다.
* **Mixtral 8x7B (양자화 버전)**: Mixture of Experts (MoE) 구조를 가진 모델로, 47B 파라미터이지만 추론 시에는 13B 파라미터만 사용합니다. `Q4_0` (약 29GB)는 VRAM이 부족하지만, `Q3_K_M` (약 20GB)도 VRAM이 부족합니다. 하지만 **`Mixtral-8x7B`의 `GGUF` 또는 `AWQ` 포맷 중 가장 낮은 양자화**를 선택하면 일부가 RAM으로 오프로드(Offload)되어 실행 가능합니다. **하지만 속도가 느려질 수 있습니다.**

#### 2) Embedding (임베딩 모델)

임베딩 모델은 비교적 작아 8GB VRAM에 쉽게 올릴 수 있습니다.

* **nomic-embed-text**: 137M 파라미터로, 크기가 매우 작고 성능이 뛰어난 모델입니다. Ollama 라이브러리에서 기본적으로 추천하는 모델입니다.
* **BGE-small-en-v1.5**: 330M 파라미터의 임베딩 모델로, 텍스트 임베딩 벤치마크에서 높은 성능을 보입니다.
* **mxbai-embed-large**: 2.7B 파라미터로, 위 두 모델보다 크지만 뛰어난 성능을 제공합니다. 8GB VRAM에도 충분히 올릴 수 있습니다.

#### 3) VLM (비전 언어 모델)

VLM은 이미지 처리를 위해 더 많은 VRAM을 요구합니다. 8GB VRAM으로는 제한적인 VLM 모델만 사용할 수 있습니다.

* **llava:7b**: LLM 모델인 Llama 7B에 비전 기능을 추가한 모델입니다. 비전 태스크는 추가적인 메모리를 사용하므로 7B 모델도 버거울 수 있습니다. `Q4` 양자화 버전을 사용해야만 실행이 가능할 것으로 보입니다.

**팁**: Ollama에서 `ollama run <모델명>:<태그>`를 입력하여 다양한 양자화 버전을 시도해 볼 수 있습니다. 만약 VRAM이 부족하다면 `Q4` -> `Q3` 순으로 낮은 양자화 버전을 시도해 보세요.

