# AI 파레토 최전선을 소유하다 — Jeff Dean

*원문: "Owning the AI Pareto Frontier — Jeff Dean"*

이 게시물을 웹에서 보기: https://www.latent.space/p/jeffdean

2000년대 초반에 Google의 검색 스택을 재작성하는 것부터 희소성이 있는 수 조(Trillion) 개의 매개변수 모델을 부활시키고 최전선 ML 연구와 함께 TPU를 공동 설계하는 것까지, Jeff Dean은 현대 AI 스택의 거의 모든 계층을 조용히 형성해 왔습니다. Google의 최고 AI 과학자이자 Gemini의 원동력인 Jeff는 CPU 및 샤딩된 인덱스부터 텍스트, 비디오, 코드를 넘나들며 추론하는 다중 모드 모델에 이르기까지 여러 스케일링 혁명을 경험했습니다.

Jeff는 "파레토 최전선을 소유한다"는 것이 실제로 무엇을 의미하는지, 왜 증류(distillation)가 모든 Flash 모델 혁신의 엔진인지, FLOPs(부동소수점 연산)가 아닌 에너지(피코줄 단위)가 어떻게 진정한 병목 현상이 되어가고 있는지, Google의 모든 AI 팀을 통합하기 위한 노력을 이끄는 것이 어떠했는지, 그리고 다음 도약이 더 큰 컨텍스트 창에서만 오는 것이 아니라 수 조 개의 토큰에 주의를 기울이는 환상을 주는 시스템에서 올 것이라고 말합니다.

## 우리가 논의한 내용

- **1990년 Jeff의 초기 신경망 논문**: 아직 멋지지 않았던 병렬 훈련, 수십 년 일찍 스케일링이 승리할 것이라고 믿었던 이유, 15년 동안 지속된 "더 큰 모델, 더 많은 데이터, 더 나은 결과"라는 만트라.

- **Google 검색의 진화**: 샤딩(sharding), 2001년에 전체 인덱스를 메모리에 넣기, LLM 이전의 쿼리 의미론 완화, 검색 파이프라인이 이미 최신 LLM 시스템과 유사한 이유.

- **파레토 최전선 전략**: 최전선 "Pro" 모델과 낮은 지연 시간 "Flash" 모델이 모두 필요한 이유, 증류가 어떻게 더 작은 모델이 이전 세대를 능가하도록 하는지.

- **증류 심층 분석**: 앙상블 → 압축 → 소프트 지도(soft supervision)로서의 로짓(logits), 그리고 가장 작은 모델을 좋게 만들기 위해 가장 큰 모델이 필요한 이유.

- **최우선 목표로서의 지연 시간**: 10~50배 낮은 지연 시간이 사용자 경험(UX)을 완전히 바꾸는 이유, 그리고 미래의 추론 작업이 어떻게 초당 10,000 토큰을 요구할 것인지.

- **에너지 기반 사고**: 비트당 피코줄, 데이터 이동 비용이 곱하기(multiply)보다 1000배 더 드는 이유, 에너지를 통한 배치(batching)의 관점, 상각(amortization)으로서의 투기적 디코딩(speculative decoding).

- **TPU 공동 설계**: 2~6년 후의 ML 작업 예측, 투기적 하드웨어 기능, 정밀도 감소, 희소성(sparsity), 모델 아키텍처와 실리콘 간의 지속적인 피드백 루프.

- **희소 모델 및 "터무니없이 거대한" 네트워크**: 1~5% 활성화의 수 조 개의 매개변수, 희소성이 항상 올바른 추상화였던 이유.

- **통합 모델 대 전문 모델**: 상징적 시스템(symbolic systems) 포기, 일반 다중 모드 모델이 수직 사일로를 지배하는 경향이 있는 이유, 수직 미세 조정이 여전히 의미 있는 경우.

- **긴 컨텍스트와 규모의 환상**: '건초 더미에서 바늘 찾기(needle-in-a-haystack)' 벤치마크를 넘어 수 조 개의 토큰을 117개의 관련 문서로 좁히는 시스템으로.

- **개인화된 AI**: 이메일, 사진, 문서에 대한 주의(허가 하에), 검색 + 추론이 어떻게 매우 개인적인 비서를 구현할 것인지.

- **코딩 에이전트**: 50명의 AI 인턴, 새로운 핵심 기술로서의 명확한 사양(crisp specifications), 초저지연 시간이 인간-에이전트 협업을 어떻게 재구성할 것인지.

- **아이디어가 여전히 중요한 이유**: 트랜스포머, 희소성, RL(강화 학습), 하드웨어, 시스템 — 스케일링은 맹목적이지 않았습니다. 조각들이 함께 곱해져야 했습니다.

## 쇼 노트

- [Gemma 3 논문](https://substack.com/redirect/3f1a98eb-edf4-4ddb-8296-975978006384?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Gemma 3](https://substack.com/redirect/cf0783a8-1644-4387-89ac-5dc9e0f42c88?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Gemini 2.5 보고서](https://substack.com/redirect/8c1f4059-f806-4e7e-84be-1e019fe17005?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Jeff Dean의 "대규모 분산 시스템 구축에 대한 소프트웨어 엔지니어링 조언" 프레젠테이션 (어림 계산 포함)](https://substack.com/redirect/8e8fff88-599d-4225-a5d0-10124c133bef?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [모든 프로그래머가 알아야 할 지연 시간 수치 (Jeff Dean)](https://substack.com/redirect/8aa42b82-0653-4534-ada0-a784d853ee31?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Jeff Dean 팩트](https://substack.com/redirect/a156faba-8cfd-495b-a55b-7b87cd298443?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Jeff Dean Google 약력](https://substack.com/redirect/6be4bd26-526c-4fbf-9722-f815bfc7cc67?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Jeff Dean, Stanford AI Club에서 "중요한 AI 동향"에 대해](https://substack.com/redirect/e89163f1-77b6-4068-a68b-6d50f146a367?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- [Jeff Dean & Noam Shazeer — Google에서 25년 (Dwarkesh)](https://substack.com/redirect/d7fd0cae-827c-468d-a56c-b036151fd14b?j=eyJ1IjoiMWxhdmY1In0.MZvdRG5O-0hDGdX7o8xcjqlaC7cnJzwOWRjmezop1Po)
- Jeff Dean LinkedIn: https://www.linkedin.com/in/jeff-dean-8b212555
- X: https://x.com/jeffdean
- Google: https://google.com
- DeepMind: https://deepmind.google

## 전체 비디오 에피소드 타임스탬프

- **00:00:04** — 소개: Alessio와 Swyx가 Google의 최고 AI 과학자 Jeff Dean을 Latent Space 팟캐스트에 환영합니다.
- **00:00:30** — 파레토 최전선 소유 및 프론티어 모델과 저지연 모델 균형 잡기
- **00:01:31** — 프론티어 모델 대 플래시 모델 + 증류의 역할
- **00:03:52** — 증류의 역사와 원래 동기
- **00:05:09** — 현대 모델 스케일링에서 증류의 역할
- **00:07:02** — 모델 계층 구조 (Flash, Pro, Ultra) 및 증류 소스
- **00:07:46** — Flash 모델 경제성과 광범위한 배포
- **00:08:10** — 복잡한 작업에서 지연 시간의 중요성
- **00:09:19** — 일부 작업의 포화 및 미래의 프론티어 작업
- **00:11:26** — 벤치마크, 공개 대 내부
- **00:12:53** — 예시 장문 컨텍스트 벤치마크 및 한계
- **00:15:01** — 장문 컨텍스트 목표: 수 조 개의 토큰에 주의 기울이기
- **00:16:26** — 순수 언어를 넘어선 현실적인 사용 사례
- **00:18:04** — 다중 모드 추론 및 비텍스트 모드
- **00:19:05** — 비전 및 모션 모드의 중요성
- **00:20:11** — 비디오 이해 예시 (구조화된 정보 추출)
- **00:20:47** — LLM 검색을 위한 검색 순위 비유
- **00:23:08** — LLM 표현 대 키워드 검색
- **00:24:06** — 초기 Google 검색 진화 및 인메모리 인덱스
- **00:26:47** — 확장 가능한 시스템을 위한 설계 원칙
- **00:28:55** — 실시간 인덱스 업데이트 및 재크롤링 전략
- **00:30:06** — 고전적인 "모든 프로그래머가 알아야 할 지연 시간 수치"
- **00:32:09** — 메모리 대 컴퓨팅 비용 및 에너지 강조
- **00:34:33** — TPU 및 모델 서빙을 위한 하드웨어 트레이드오프
- **00:35:57** — TPU 설계 결정 및 ML과의 공동 설계
- **00:38:06** — 모델 아키텍처를 하드웨어에 맞추기
- **00:39:50** — 대안: 에너지 기반 모델, 투기적 디코딩
- **00:42:21** — 열린 연구 방향: 복잡한 워크플로우, RL
- **00:44:56** — 검증 불가능한 RL 도메인 및 모델 평가
- **00:46:13** — 상징적 시스템에서 통합 LLM으로의 전환
- **00:47:59** — 통합 모델 대 전문 모델
- **00:50:38** — 지식 대 추론 및 검색 + 추론
- **00:52:24** — 수직 모델 전문화 및 모듈
- **00:55:21** — 수직 도메인을 위한 토큰 수 고려 사항
- **00:56:09** — 저자원 언어 및 상황별 학습
- **00:59:22** — 기원: Dean의 초기 신경망 작업
- **01:10:07** — 코딩을 위한 AI 및 인간-모델 상호 작용 스타일
- **01:15:52** — 코딩 에이전트를 위한 명확한 사양의 중요성
- **01:19:23** — 예측: 개인화된 모델 및 상태 검색
- **01:22:36** — 초당 토큰 목표 (10k 이상) 및 추론 처리량
- **01:23:20** — 에피소드 결론 및 감사

---

*추가로 궁금한 점이 있으시면 언제든지 문의하세요.*
